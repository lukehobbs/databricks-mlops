{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "- Install required libraries\n",
    "- Configure databricks secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlflow\n",
    "%pip install xgboost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local config\n",
    "\n",
    "When running locally you'll need to set these environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['DATABRICKS_HOST']=\"<redacted>\"\n",
    "# os.environ['DATABRICKS_TOKEN']=\"<redacted>\"\n",
    "# os.environ['DATABRICKS_USERNAME']=\"<redacted>\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure MLFlow to log to Databricks\n",
    "- Create/Update an experiment in Databricks\n",
    "- Enable autolog for xgboost so metrics/params are logged automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_URI']=\"databricks\"\n",
    "\n",
    "databricks_username=os.environ['DATABRICKS_USERNAME']\n",
    "\n",
    "experiment_path=\"/Users/{}/Experiments/mlops-experiment-1\".format(databricks_username)\n",
    "\n",
    "mlflow.set_experiment(experiment_path)\n",
    "mlflow.xgboost.autolog(log_input_examples=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n",
    "# may not use this file except in compliance with the License. A copy of\n",
    "# the License is located at\n",
    "#\n",
    "#     http://aws.amazon.com/apache2.0/\n",
    "#\n",
    "# or in the \"license\" file accompanying this file. This file is\n",
    "# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n",
    "# ANY KIND, either express or implied. See the License for the specific\n",
    "# language governing permissions and limitations under the License.\n",
    "\"\"\"Feature engineers the customer churn dataset.\"\"\"\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"Starting preprocessing.\")\n",
    "\n",
    "input_data_path = os.path.join(\"data/ml/processing/input\", \"churn.csv\")\n",
    "\n",
    "logger.info(\"Reading input data\")\n",
    "\n",
    "# read csv\n",
    "df = pd.read_csv(input_data_path)\n",
    "\n",
    "# drop the \"Phone\" feature column\n",
    "df = df.drop([\"Phone\"], axis=1)\n",
    "\n",
    "# Change the data type of \"Area Code\"\n",
    "df[\"Area Code\"] = df[\"Area Code\"].astype(object)\n",
    "\n",
    "# Drop several other columns\n",
    "df = df.drop([\"Day Charge\", \"Eve Charge\", \"Night Charge\", \"Intl Charge\"], axis=1)\n",
    "\n",
    "# Convert categorical variables into dummy/indicator variables.\n",
    "model_data = pd.get_dummies(df)\n",
    "\n",
    "# Create one binary classification target column\n",
    "model_data = pd.concat(\n",
    "    [\n",
    "        model_data[\"Churn?_True.\"],\n",
    "        model_data.drop([\"Churn?_False.\", \"Churn?_True.\"], axis=1),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Get random split sections\n",
    "train_data_boundary = random.random() * len(model_data)\n",
    "remaining = len(model_data) - train_data_boundary\n",
    "test_data_boundary = len(model_data) - (random.random() * remaining)\n",
    "data_splits = [int(train_data_boundary), int(test_data_boundary)]\n",
    "\n",
    "# Split the data\n",
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), data_splits)\n",
    "\n",
    "if os.path.exists(\"/dbfs\"):\n",
    "    Path(\"/dbfs/{}/train\".format(databricks_username)).mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"/dbfs/{}/test\".format(databricks_username)).mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"/dbfs/{}/validation\".format(databricks_username)).mkdir(parents=True, exist_ok=True)\n",
    "    Path(\"/dbfs/{}/output\".format(databricks_username)).mkdir(parents=True, exist_ok=True)\n",
    "    output_data_path = \"/dbfs/{}\".format(databricks_username)\n",
    "else:\n",
    "    output_data_path = \"data/ml/processing/\"\n",
    "\n",
    "train_data.to_csv(\"{}/train/train.csv\".format(output_data_path), header=False, index=False)\n",
    "validation_data.to_csv(\"{}/validation/validation.csv\".format(output_data_path), header=False, index=False)\n",
    "test_data.to_csv(\"{}/test/test.csv\".format(output_data_path), header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "\n",
    "X_train = train_data.drop(\"Churn?_True.\", axis=1)\n",
    "y_train = train_data[\"Churn?_True.\"]\n",
    "\n",
    "X_test = test_data.drop(\"Churn?_True.\", axis=1)\n",
    "y_test = test_data[\"Churn?_True.\"]\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mlflow.log_params({\n",
    "        \"data_splits\": data_splits\n",
    "    })\n",
    "\n",
    "    mlflow.log_metrics({\n",
    "        \"acc\": accuracy_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred)\n",
    "    })\n",
    "\n",
    "    np.savetxt(\"{}/output/predictions.csv\".format(output_data_path), y_pred)\n",
    "    mlflow.log_artifact(\"{}/output/predictions.csv\".format(output_data_path))\n",
    "    mlflow.log_artifact(\"{}/train/train.csv\".format(output_data_path))\n",
    "    mlflow.log_artifact(\"{}/test/test.csv\".format(output_data_path))\n",
    "    mlflow.log_artifact(\"{}/validation/validation.csv\".format(output_data_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
